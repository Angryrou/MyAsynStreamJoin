package org.apache.spark.streaming

/**
  * Created by kawhi on 2017/5/30.
  */

import org.apache.spark.{HashPartitioner, Partitioner}
import org.apache.spark.annotation.Experimental
import org.apache.spark.api.java.{JavaPairRDD, JavaUtils, Optional}
import org.apache.spark.api.java.function.{Function3 => JFunction3, Function4 => JFunction4}
import org.apache.spark.rdd.RDD
import org.apache.spark.util.ClosureCleaner
import org.apache.spark.streaming.api.java.{Function5 => JFunction5}

/**
  * :: Experimental ::
  * Abstract class representing all the specifications of the DStream transformation
  * `mapWithState` operation of a
  * [[org.apache.spark.streaming.dstream.PairDStreamFunctions pair DStream]] (Scala) or a
  * [[org.apache.spark.streaming.api.java.JavaPairDStream JavaPairDStream]] (Java).
  * Use [[org.apache.spark.streaming.MyStateSpecWithIndex.function() MyStateSpecWithIndex.function]] factory methods
  * to create instances of this class.
  *
  * Example in Scala:
  * {{{
  *    // A mapping function that maintains an integer state and return a String
  *    def mappingFunction(key: String, value: Option[Int], state: State[Int]): Option[String] = {
  *      // Use state.exists(), state.get(), state.update() and state.remove()
  *      // to manage state, and return the necessary string
  *    }
  *
  *    val spec = MyStateSpecWithIndex.function(mappingFunction).numPartitions(10)
  *
  *    val mapWithStateDStream = keyValueDStream.mapWithState[StateType, MappedType](spec)
  * }}}
  *
  * Example in Java:
  * {{{
  *   // A mapping function that maintains an integer state and return a string
  *   Function3<String, Optional<Integer>, State<Integer>, String> mappingFunction =
  *       new Function3<String, Optional<Integer>, State<Integer>, String>() {
  *           @Override
  *           public Optional<String> call(Optional<Integer> value, State<Integer> state) {
  *               // Use state.exists(), state.get(), state.update() and state.remove()
  *               // to manage state, and return the necessary string
  *           }
  *       };
  *
  *    JavaMapWithStateDStream<String, Integer, Integer, String> mapWithStateDStream =
  *        keyValueDStream.mapWithState(MyStateSpecWithIndex.function(mappingFunc));
  * }}}
  *
  * @tparam KeyType    Class of the state key
  * @tparam ValueType  Class of the state value
  * @tparam StateType  Class of the state data
  * @tparam MappedType Class of the mapped elements
  */
@Experimental
sealed abstract class MyStateSpecWithIndex[KeyType, ValueType, StateType, MappedType] extends Serializable {

  /** Set the RDD containing the initial states that will be used by `mapWithState` */
  def initialState(rdd: RDD[(KeyType, StateType)]): this.type

  /** Set the RDD containing the initial states that will be used by `mapWithState` */
  def initialState(javaPairRDD: JavaPairRDD[KeyType, StateType]): this.type

  /**
    * Set the number of partitions by which the state RDDs generated by `mapWithState`
    * will be partitioned. Hash partitioning will be used.
    */
  def numPartitions(numPartitions: Int): this.type

  /**
    * Set the partitioner by which the state RDDs generated by `mapWithState` will be partitioned.
    */
  def partitioner(partitioner: Partitioner): this.type

  /**
    * Set the duration after which the state of an idle key will be removed. A key and its state is
    * considered idle if it has not received any data for at least the given duration. The
    * mapping function will be called one final time on the idle states that are going to be
    * removed; [[org.apache.spark.streaming.State State.isTimingOut()]] set
    * to `true` in that call.
    */
  def timeout(idleDuration: Duration): this.type
}


/**
  * :: Experimental ::
  * Builder object for creating instances of [[org.apache.spark.streaming.MyStateSpecWithIndex MyStateSpecWithIndex]]
  * that is used for specifying the parameters of the DStream transformation `mapWithState`
  * that is used for specifying the parameters of the DStream transformation
  * `mapWithState` operation of a
  * [[org.apache.spark.streaming.dstream.PairDStreamFunctions pair DStream]] (Scala) or a
  * [[org.apache.spark.streaming.api.java.JavaPairDStream JavaPairDStream]] (Java).
  *
  * Example in Scala:
  * {{{
  *    // A mapping function that maintains an integer state and return a String
  *    def mappingFunction(key: String, value: Option[Int], state: State[Int]): Option[String] = {
  *      // Use state.exists(), state.get(), state.update() and state.remove()
  *      // to manage state, and return the necessary string
  *    }
  *
  *    val spec = MyStateSpecWithIndex.function(mappingFunction).numPartitions(10)
  *
  *    val mapWithStateDStream = keyValueDStream.mapWithState[StateType, MappedType](spec)
  * }}}
  *
  * Example in Java:
  * {{{
  *   // A mapping function that maintains an integer state and return a string
  *   Function3<String, Optional<Integer>, State<Integer>, String> mappingFunction =
  *       new Function3<String, Optional<Integer>, State<Integer>, String>() {
  *           @Override
  *           public Optional<String> call(Optional<Integer> value, State<Integer> state) {
  *               // Use state.exists(), state.get(), state.update() and state.remove()
  *               // to manage state, and return the necessary string
  *           }
  *       };
  *
  *    JavaMapWithStateDStream<String, Integer, Integer, String> mapWithStateDStream =
  *        keyValueDStream.mapWithState(MyStateSpecWithIndex.function(mappingFunc));
  *}}}
  */
@Experimental
object MyStateSpecWithIndex {
  /**
    * Create a [[org.apache.spark.streaming.MyStateSpecWithIndex MyStateSpecWithIndex]] for setting all the specifications
    * of the `mapWithState` operation on a
    * [[org.apache.spark.streaming.dstream.PairDStreamFunctions pair DStream]].
    *
    * @param mappingFunction The function applied on every data item to manage the associated state
    *                         and generate the mapped data
    * @tparam KeyType      Class of the keys
    * @tparam ValueType    Class of the values
    * @tparam StateType    Class of the states data
    * @tparam MappedType   Class of the mapped data
    */
  def function[KeyType, ValueType, StateType, MappedType](
                                                           mappingFunction: (Int, Time, KeyType, Option[ValueType], State[StateType]) => Option[MappedType]
                                                         ): MyStateSpecWithIndex[KeyType, ValueType, StateType, MappedType] = {
    ClosureCleaner.clean(mappingFunction, checkSerializable = true)
    new MyStateSpecWithIndexImpl(mappingFunction)
  }

  /**
    * Create a [[org.apache.spark.streaming.MyStateSpecWithIndex MyStateSpecWithIndex]] for setting all the specifications
    * of the `mapWithState` operation on a
    * [[org.apache.spark.streaming.dstream.PairDStreamFunctions pair DStream]].
    *
    * @param mappingFunction The function applied on every data item to manage the associated state
    *                         and generate the mapped data
    * @tparam ValueType    Class of the values
    * @tparam StateType    Class of the states data
    * @tparam MappedType   Class of the mapped data
    */
  def function[KeyType, ValueType, StateType, MappedType](
                                                           mappingFunction: (Int, KeyType, Option[ValueType], State[StateType]) => MappedType
                                                         ): MyStateSpecWithIndex[KeyType, ValueType, StateType, MappedType] = {
    ClosureCleaner.clean(mappingFunction, checkSerializable = true)
    val wrappedFunction =
      (index: Int, time: Time, key: KeyType, value: Option[ValueType], state: State[StateType]) => {
        Some(mappingFunction(index, key, value, state))
      }
    new MyStateSpecWithIndexImpl(wrappedFunction)
  }

  /**
    * Create a [[org.apache.spark.streaming.MyStateSpecWithIndex MyStateSpecWithIndex]] for setting all
    * the specifications of the `mapWithState` operation on a
    * [[org.apache.spark.streaming.api.java.JavaPairDStream JavaPairDStream]].
    *
    * @param mappingFunction The function applied on every data item to manage the associated
    *                        state and generate the mapped data
    * @tparam KeyType      Class of the keys
    * @tparam ValueType    Class of the values
    * @tparam StateType    Class of the states data
    * @tparam MappedType   Class of the mapped data
    */
  def function[KeyType, ValueType, StateType, MappedType](mappingFunction:
                                                          JFunction5[Int, Time, KeyType, Optional[ValueType], State[StateType], Optional[MappedType]]):
  MyStateSpecWithIndex[KeyType, ValueType, StateType, MappedType] = {
    val wrappedFunc = (index: Int, time: Time, k: KeyType, v: Option[ValueType], s: State[StateType]) => {
      val t = mappingFunction.call(index, time, k, JavaUtils.optionToOptional(v), s)
      if (t.isPresent) {
        Some(t.get)
      } else {
        None
      }
    }
    MyStateSpecWithIndex.function(wrappedFunc)
  }

  /**
    * Create a [[org.apache.spark.streaming.MyStateSpecWithIndex MyStateSpecWithIndex]] for setting all the specifications
    * of the `mapWithState` operation on a
    * [[org.apache.spark.streaming.api.java.JavaPairDStream JavaPairDStream]].
    *
    * @param mappingFunction The function applied on every data item to manage the associated
    *                        state and generate the mapped data
    * @tparam ValueType    Class of the values
    * @tparam StateType    Class of the states data
    * @tparam MappedType   Class of the mapped data
    */
  def function[KeyType, ValueType, StateType, MappedType](
                                                           mappingFunction: JFunction4[Int, KeyType, Optional[ValueType], State[StateType], MappedType]):
  MyStateSpecWithIndex[KeyType, ValueType, StateType, MappedType] = {
    val wrappedFunc = (index: Int, k: KeyType, v: Option[ValueType], s: State[StateType]) => {
      mappingFunction.call(index, k, JavaUtils.optionToOptional(v), s)
    }
    MyStateSpecWithIndex.function(wrappedFunc)
  }
}


/** Internal implementation of [[org.apache.spark.streaming.MyStateSpecWithIndex]] interface. */
private[streaming]
case class MyStateSpecWithIndexImpl[K, V, S, T](
                                      function: (Int, Time, K, Option[V], State[S]) => Option[T]) extends MyStateSpecWithIndex[K, V, S, T] {

  require(function != null)

  @volatile private var partitioner: Partitioner = null
  @volatile private var initialStateRDD: RDD[(K, S)] = null
  @volatile private var timeoutInterval: Duration = null

  override def initialState(rdd: RDD[(K, S)]): this.type = {
    this.initialStateRDD = rdd
    this
  }

  override def initialState(javaPairRDD: JavaPairRDD[K, S]): this.type = {
    this.initialStateRDD = javaPairRDD.rdd
    this
  }

  override def numPartitions(numPartitions: Int): this.type = {
    this.partitioner(new HashPartitioner(numPartitions))
    this
  }

  override def partitioner(partitioner: Partitioner): this.type = {
    this.partitioner = partitioner
    this
  }

  override def timeout(interval: Duration): this.type = {
    this.timeoutInterval = interval
    this
  }

  // ================= Private Methods =================

  private[streaming] def getFunction(): (Int, Time, K, Option[V], State[S]) => Option[T] = function

  private[streaming] def getInitialStateRDD(): Option[RDD[(K, S)]] = Option(initialStateRDD)

  private[streaming] def getPartitioner(): Option[Partitioner] = Option(partitioner)

  private[streaming] def getTimeoutInterval(): Option[Duration] = Option(timeoutInterval)
}
